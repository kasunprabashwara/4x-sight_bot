{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-01-13T08:53:33.156884Z",
     "iopub.status.busy": "2025-01-13T08:53:33.156468Z",
     "iopub.status.idle": "2025-01-13T08:53:37.275508Z",
     "shell.execute_reply": "2025-01-13T08:53:37.273987Z",
     "shell.execute_reply.started": "2025-01-13T08:53:33.156839Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install tianshou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-13T08:53:37.277696Z",
     "iopub.status.busy": "2025-01-13T08:53:37.277273Z",
     "iopub.status.idle": "2025-01-13T08:53:37.283760Z",
     "shell.execute_reply": "2025-01-13T08:53:37.282592Z",
     "shell.execute_reply.started": "2025-01-13T08:53:37.277659Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import torch as th\n",
    "import torch.nn as nn\n",
    "import gymnasium as gym\n",
    "from gymnasium import Env\n",
    "from gymnasium.spaces import Discrete, Box\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-13T08:53:37.286632Z",
     "iopub.status.busy": "2025-01-13T08:53:37.286302Z",
     "iopub.status.idle": "2025-01-13T08:53:37.301648Z",
     "shell.execute_reply": "2025-01-13T08:53:37.300553Z",
     "shell.execute_reply.started": "2025-01-13T08:53:37.286604Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def get_forex_data():\n",
    "    # Load the dataset\n",
    "    data_set = pd.read_csv('/kaggle/input/train-dataset/Foreign_Exchange_Rates.csv', na_values='ND')\n",
    "\n",
    "    # Interpolate missing values to handle missing data\n",
    "    data_set = data_set.infer_objects(copy=False)  # Ensure non-numeric columns are correctly inferred\n",
    "    data_set.interpolate(inplace=True)\n",
    "\n",
    "    # Select only the columns for EUR/USD and JPY/USD exchange rates\n",
    "    df = data_set[['EURO AREA - EURO/US$', 'JAPAN - YEN/US$']].copy()\n",
    "\n",
    "    # Add derived column for YEN/EURO exchange rate\n",
    "    df['YEN/EURO'] = df['JAPAN - YEN/US$'] / df['EURO AREA - EURO/US$']\n",
    "\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-13T08:53:37.303868Z",
     "iopub.status.busy": "2025-01-13T08:53:37.303550Z",
     "iopub.status.idle": "2025-01-13T08:53:37.327099Z",
     "shell.execute_reply": "2025-01-13T08:53:37.325610Z",
     "shell.execute_reply.started": "2025-01-13T08:53:37.303825Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class State:\n",
    "    def __init__(self):\n",
    "        self._prices = None\n",
    "        self._first_diff = None\n",
    "        self._offset = None\n",
    "        self.balance = None\n",
    "        self.portfolio = None\n",
    "        self.euro_buy_value = None\n",
    "        self.yen_buy_value = None\n",
    "        self.trade_max_percentage = None\n",
    "\n",
    "    def reset(self, prices, offset, initial_balance, trade_max_percentage ):\n",
    "        self._prices = prices\n",
    "        first_differences = prices.diff()\n",
    "        # Normalize the first differences using Min-Max normalization\n",
    "        self._first_diff =  (first_differences - first_differences.min()) / (first_differences.max() - first_differences.min())\n",
    "        self._offset = offset\n",
    "        self.balance = initial_balance\n",
    "        self.trade_max_percentage = trade_max_percentage\n",
    "        self.portfolio = {'USD': initial_balance, 'EUR': 0, 'JPY':0}\n",
    "        self.euro_buy_value = 0\n",
    "        self.yen_buy_value = 0\n",
    "\n",
    "    def step(self, action, reward_type = \"inDirect\"):\n",
    "        reward = 0\n",
    "        current_price = self._prices.iloc[self._offset][['EURO AREA - EURO/US$', 'JAPAN - YEN/US$', 'YEN/EURO']].values\n",
    "        eur_usd, jpy_usd, jpy_eur = current_price\n",
    "        max_trade_amount = self.balance * self.trade_max_percentage\n",
    "        # action is an array of floats between -1 and 1\n",
    "        # remove the looping transaction\n",
    "        if(action[0] < 0 and action[1] > 0 and action[2] < 0):\n",
    "            common = min(-action[0], action[1], -action[2])\n",
    "            action[0] += common\n",
    "            action[1] -= common\n",
    "            action[2] += common\n",
    "        elif(action[0] > 0 and action[1] < 0 and action[2] > 0):\n",
    "            common = min(action[0], -action[1], action[2])\n",
    "            action[0] -= common\n",
    "            action[1] += common\n",
    "            action[2] -= common\n",
    "        # USD and EUR, positive means buy EUR\n",
    "        if action[0] > 0:\n",
    "            trade_amount =  abs(max_trade_amount*action[0])\n",
    "            trade_volume = min(self.portfolio['USD'], trade_amount)\n",
    "            reward = 0\n",
    "\n",
    "            self.portfolio['EUR'] += trade_volume * eur_usd\n",
    "            self.portfolio['USD'] -= trade_volume\n",
    "            self.euro_buy_value += trade_volume\n",
    "        elif action[0] < 0:\n",
    "            trade_amount =  abs(max_trade_amount*action[0])\n",
    "            trade_volume = min(self.portfolio['EUR'], trade_amount * eur_usd)\n",
    "            if trade_volume > 0:\n",
    "                reward = trade_volume * (1/eur_usd-(self.euro_buy_value/self.portfolio['EUR']))\n",
    "            self.portfolio['USD'] += trade_volume / eur_usd\n",
    "            self.portfolio['EUR'] -= trade_volume\n",
    "            self.euro_buy_value -= trade_volume / eur_usd\n",
    "        # USD and YEN, positive means buy YEN\n",
    "        if action[1] > 0:\n",
    "            trade_amount =  abs(max_trade_amount*action[1])\n",
    "            trade_volume = min(self.portfolio['USD'], trade_amount)\n",
    "            reward = 0\n",
    "            self.portfolio['JPY'] += trade_volume * jpy_usd\n",
    "            self.portfolio['USD'] -= trade_volume\n",
    "            self.yen_buy_value += trade_volume\n",
    "        elif action[1] < 0:\n",
    "            trade_amount =  abs(max_trade_amount*action[1])\n",
    "            trade_volume = min(self.portfolio['JPY'], trade_amount * jpy_usd)\n",
    "            if trade_volume > 0:\n",
    "                reward = trade_volume * (1/jpy_usd - self.yen_buy_value/self.portfolio['JPY'])\n",
    "            self.portfolio['USD'] += trade_volume / jpy_usd\n",
    "            self.portfolio['JPY'] -= trade_volume\n",
    "            self.yen_buy_value -= trade_volume / jpy_usd\n",
    "        # EUR and YEN, positive means buy YEN\n",
    "        if action[2] > 0:\n",
    "            trade_amount =  abs(max_trade_amount*action[2])\n",
    "            trade_volume = min(self.portfolio['EUR'], trade_amount * eur_usd)\n",
    "            if trade_volume > 0:\n",
    "                reward = trade_volume * (1/eur_usd - self.euro_buy_value/self.portfolio['EUR'])\n",
    "\n",
    "            self.portfolio['JPY'] += trade_volume * jpy_eur\n",
    "            self.portfolio['EUR'] -= trade_volume\n",
    "            self.euro_buy_value -= trade_volume / eur_usd\n",
    "            self.yen_buy_value += trade_volume / eur_usd\n",
    "        elif action[2] < 0:\n",
    "            trade_amount =  abs(max_trade_amount*action[2])\n",
    "            trade_volume = min(self.portfolio['JPY'], trade_amount * jpy_usd)\n",
    "            if trade_volume > 0:\n",
    "                reward = trade_volume * (1/jpy_usd - self.yen_buy_value/self.portfolio['JPY'])\n",
    "\n",
    "            self.portfolio['EUR'] += trade_volume / jpy_eur\n",
    "            self.portfolio['JPY'] -= trade_volume\n",
    "            self.euro_buy_value += trade_volume / jpy_usd\n",
    "            self.yen_buy_value -= trade_volume / jpy_usd\n",
    "\n",
    "        portfolio_value = (self.portfolio['USD'] + self.portfolio['EUR'] / eur_usd + self.portfolio['JPY'] / jpy_usd)\n",
    "        if reward_type == \"Direct\":\n",
    "            reward = portfolio_value - self.balance\n",
    "        self.balance = portfolio_value\n",
    "        self._offset += 1\n",
    "        done = self._offset >= len(self._prices) - 1\n",
    "        # reward = 100*(action[0]-action[1]-action[2])\n",
    "        return reward, done\n",
    "\n",
    "    def encode(self):\n",
    "        # Extract historical prices\n",
    "        current_prices = self._first_diff.iloc[self._offset]\n",
    "        encoded_prices = np.array(current_prices[['EURO AREA - EURO/US$', 'JAPAN - YEN/US$', 'YEN/EURO']]).flatten()\n",
    "        portfolio_fraction = np.array([self.portfolio['USD'],self.portfolio['EUR'],self.portfolio['JPY']])/self.balance\n",
    "\n",
    "\n",
    "        # Combine all features into a single array\n",
    "        # use log to normalize balance\n",
    "        encoded_features = np.concatenate([\n",
    "            encoded_prices,\n",
    "            portfolio_fraction,\n",
    "            [self.euro_buy_value, self.yen_buy_value/100, self.trade_max_percentage]\n",
    "        ])\n",
    "        return encoded_features\n",
    "\n",
    "    @property\n",
    "    def shape(self):\n",
    "        # Update the shape to match the new number of encoded features\n",
    "        return (3 + 3 + 3,)  # 3 prices + 3 portfolio + 3 additional values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-13T08:53:37.328807Z",
     "iopub.status.busy": "2025-01-13T08:53:37.328383Z",
     "iopub.status.idle": "2025-01-13T08:53:37.349015Z",
     "shell.execute_reply": "2025-01-13T08:53:37.347985Z",
     "shell.execute_reply.started": "2025-01-13T08:53:37.328763Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class ForexTradingEnv(Env):\n",
    "    def __init__(self, df, initial_balance=1000):\n",
    "        super(ForexTradingEnv, self).__init__()\n",
    "        self.df = df\n",
    "        self.initial_balance = initial_balance\n",
    "        self.state = State()\n",
    "        # shape is all the currency combinations\n",
    "        self.action_space = Box(low=-1, high=1, shape=(3,), dtype=np.float32)\n",
    "        self.observation_space = Box(\n",
    "            low=0, high=np.inf, shape=self.state.shape, dtype=np.float32\n",
    "        )\n",
    "    def seed(self, seed):\n",
    "        np.random.seed(seed)\n",
    "    def reset(self,sequence_length, seed=None):\n",
    "        super().reset(seed=seed)\n",
    "        rng = np.random.default_rng(seed)\n",
    "        offset = np.random.randint(sequence_length, len(self.df) - 1)\n",
    "        self.state.reset(prices=self.df, offset=offset, initial_balance=self.initial_balance, trade_max_percentage= 1)\n",
    "        return self.state.encode()\n",
    "\n",
    "    def step(self, action):\n",
    "        reward, terminated = self.state.step(action)\n",
    "        truncated = self.state._offset >= len(self.df) - 1\n",
    "        observation = self.state.encode()\n",
    "        info = {\n",
    "            \"balance\": self.state.balance,  # Include the current balance\n",
    "            # Add any other relevant fields from the State object if needed\n",
    "        }\n",
    "        return observation, reward, terminated, truncated, info\n",
    "\n",
    "    def render(self, mode='human'):\n",
    "        if mode != 'human':\n",
    "            raise NotImplementedError(\"Only 'human' rendering mode is supported.\")\n",
    "        print(f\"Step: {self.state._offset}\")\n",
    "        print(f\"Portfolio: {self.state.portfolio}\")\n",
    "        print(f\"Balance: {self.state.balance}\")\n",
    "class SequenceEnvironment(gym.Env):\n",
    "    def __init__(self, df, sequence_length):\n",
    "        super(SequenceEnvironment, self).__init__()\n",
    "        self.original_env = ForexTradingEnv(df)\n",
    "        self.sequence_length = sequence_length\n",
    "        self.buffer = []\n",
    "        self.observation_space = Box(\n",
    "            low=np.repeat(self.original_env.observation_space.low[None, :], sequence_length, axis=0),\n",
    "            high=np.repeat(self.original_env.observation_space.high[None, :], sequence_length, axis=0),\n",
    "            dtype=self.original_env.observation_space.dtype\n",
    "        )\n",
    "        self.action_space = self.original_env.action_space\n",
    "\n",
    "    def reset(self,seed=None):\n",
    "        obs = self.original_env.reset(self.sequence_length)\n",
    "        self.buffer = [obs] * self.sequence_length\n",
    "        return np.array(self.buffer), {}\n",
    "\n",
    "    def step(self, action):\n",
    "        obs, reward, terminated, truncated, info = self.original_env.step(action)\n",
    "        self.buffer.pop(0)\n",
    "        self.buffer.append(obs)\n",
    "        return np.array(self.buffer), reward, terminated, truncated, info\n",
    "        \n",
    "    def get_balance(self):\n",
    "        return self.original_env.state.balance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-13T08:53:37.351164Z",
     "iopub.status.busy": "2025-01-13T08:53:37.350699Z",
     "iopub.status.idle": "2025-01-13T08:53:37.369349Z",
     "shell.execute_reply": "2025-01-13T08:53:37.368449Z",
     "shell.execute_reply.started": "2025-01-13T08:53:37.351124Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class LSTMFeatureExtractor(nn.Module):\n",
    "    def __init__(self, input_dim, lstm_hidden_size=128, features_dim=256):\n",
    "        super().__init__()\n",
    "        self.device= \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        self.lstm_hidden_size = lstm_hidden_size\n",
    "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=lstm_hidden_size, batch_first=True)\n",
    "        self.linear1 = nn.Linear(lstm_hidden_size, 128)\n",
    "        self.linear2 = nn.Linear(128, features_dim)\n",
    "\n",
    "    def forward(self, x, hidden_state=None):\n",
    "        if isinstance(x, np.ndarray):\n",
    "            x = torch.tensor(x, dtype=torch.float32, device=self.device)\n",
    "        if hidden_state is None:\n",
    "            h_0 = torch.zeros(1, x.size(0), self.lstm_hidden_size).to(self.device)\n",
    "            c_0 = torch.zeros(1, x.size(0), self.lstm_hidden_size).to(self.device)\n",
    "        else:\n",
    "            h_0, c_0 = hidden_state\n",
    "\n",
    "        lstm_out, (h_n, c_n) = self.lstm(x, (h_0, c_0))\n",
    "        last_time_step_out = lstm_out[:, -1, :]\n",
    "        x = F.relu(self.linear1(last_time_step_out))\n",
    "        features = F.relu(self.linear2(x))\n",
    "        return features, (h_n, c_n)\n",
    "\n",
    "class ActorLSTM(nn.Module):\n",
    "    def __init__(self, input_dim, lstm_hidden_size=128, features_dim=256, action_dim=3):\n",
    "        super().__init__()\n",
    "        self.device= \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        self.feature_extractor = LSTMFeatureExtractor(input_dim, lstm_hidden_size, features_dim)\n",
    "        self.actor_head = nn.Linear(features_dim, action_dim)\n",
    "        self.log_std = nn.Parameter(torch.zeros(action_dim))\n",
    "\n",
    "    def forward(self, obs, state=None, **kwargs):\n",
    "        if isinstance(obs, np.ndarray):\n",
    "            obs = torch.tensor(obs, dtype=torch.float32, device=self.device)\n",
    "        features, state = self.feature_extractor(obs, state)\n",
    "        logits = self.actor_head(features)\n",
    "        return logits, state\n",
    "\n",
    "\n",
    "class CriticLSTM(nn.Module):\n",
    "    def __init__(self, input_dim, lstm_hidden_size=128, features_dim=256):\n",
    "        super().__init__()\n",
    "        self.device= \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        self.feature_extractor = LSTMFeatureExtractor(input_dim, lstm_hidden_size, features_dim)\n",
    "        self.critic_head = nn.Linear(features_dim, 1)\n",
    "\n",
    "    def forward(self, obs, state=None, **kwargs):\n",
    "        if isinstance(obs, np.ndarray):\n",
    "            obs = torch.tensor(obs, dtype=torch.float32, device=self.device)\n",
    "        features, state = self.feature_extractor(obs, state)\n",
    "        value = self.critic_head(features)\n",
    "        return value, state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-13T08:53:37.370936Z",
     "iopub.status.busy": "2025-01-13T08:53:37.370508Z",
     "iopub.status.idle": "2025-01-13T08:53:37.488339Z",
     "shell.execute_reply": "2025-01-13T08:53:37.487174Z",
     "shell.execute_reply.started": "2025-01-13T08:53:37.370900Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from tianshou.policy import PPOPolicy\n",
    "import torch\n",
    "from torch.optim import Adam\n",
    "from tianshou.env import DummyVectorEnv\n",
    "from torch.distributions import Independent, Normal\n",
    "from tianshou.utils.net.common import ActorCritic, Net\n",
    "from tianshou.utils.net.continuous import ActorProb, Critic\n",
    "data =  get_forex_data()\n",
    "# Initialize your environment\n",
    "seed = 434\n",
    "device= \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "env = SequenceEnvironment(df=data, sequence_length=60)\n",
    "envs = DummyVectorEnv([lambda: SequenceEnvironment(df=data, sequence_length=60)] * 10)\n",
    "# Define network dimensions\n",
    "input_dim = env.observation_space.shape[1]  # Number of features per time step\n",
    "action_dim = env.action_space.shape[0]      # Number of possible actions\n",
    "preprocess_output_dim = 256\n",
    "\n",
    "# actor = ActorLSTM(input_dim=input_dim, lstm_hidden_size=128, features_dim=256, action_dim=action_dim)\n",
    "# critic = CriticLSTM(input_dim=input_dim, lstm_hidden_size=128, features_dim=256)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "envs.seed(seed)\n",
    "# model\n",
    "net = LSTMFeatureExtractor(input_dim=input_dim,features_dim=preprocess_output_dim)\n",
    "actor = ActorProb(net, action_dim, unbounded=True, device=device, preprocess_net_output_dim=preprocess_output_dim).to(device)\n",
    "critic = Critic(net,device=device,preprocess_net_output_dim=preprocess_output_dim).to(device)\n",
    "actor_critic = ActorCritic(actor, critic)\n",
    "# orthogonal initialization\n",
    "for m in actor_critic.modules():\n",
    "    if isinstance(m, torch.nn.Linear):\n",
    "        torch.nn.init.orthogonal_(m.weight)\n",
    "        torch.nn.init.zeros_(m.bias)\n",
    "def dist(loc_scale: tuple[torch.Tensor, torch.Tensor]):\n",
    "    loc, scale = loc_scale\n",
    "    return Independent(Normal(loc, scale), 1)\n",
    "optim = torch.optim.Adam(actor_critic.parameters(), lr=3e-4)\n",
    "policy = PPOPolicy(\n",
    "    actor=actor,\n",
    "    critic=critic,\n",
    "    optim=optim,\n",
    "    dist_fn=dist,\n",
    "    action_space=env.action_space,\n",
    "    max_grad_norm=0.5,\n",
    "    discount_factor=1,\n",
    "    gae_lambda=0.95,\n",
    "    vf_coef=0.5,\n",
    "    ent_coef=0.01,\n",
    "    action_bound_method=\"clip\",\n",
    "    reward_normalization=True,\n",
    "    dual_clip=None,\n",
    "    value_clip=True,\n",
    "    advantage_normalization=True,\n",
    "    recompute_advantage=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-13T08:53:37.490753Z",
     "iopub.status.busy": "2025-01-13T08:53:37.490420Z",
     "iopub.status.idle": "2025-01-13T09:07:08.122871Z",
     "shell.execute_reply": "2025-01-13T09:07:08.121310Z",
     "shell.execute_reply.started": "2025-01-13T08:53:37.490726Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from tianshou.env import DummyVectorEnv\n",
    "from tianshou.data import Collector, VectorReplayBuffer\n",
    "from tianshou.policy import PPOPolicy\n",
    "from tianshou.trainer import OnpolicyTrainer\n",
    "from tianshou.utils import TensorboardLogger\n",
    "from torch.distributions import Independent, Normal\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tianshou.utils.net.common import ActorCritic, Net\n",
    "from tianshou.utils.net.continuous import ActorProb, Critic\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "def dist(loc,scale):\n",
    "    return Independent(Normal(loc, scale), 1)\n",
    "data = get_forex_data()\n",
    "# Environment setup\n",
    "env = SequenceEnvironment(df=data, sequence_length=60)\n",
    "train_envs = DummyVectorEnv([lambda: SequenceEnvironment(df=data, sequence_length=60)] * 10)\n",
    "test_envs = DummyVectorEnv([lambda: SequenceEnvironment(df=data, sequence_length=60)] * 10)\n",
    "\n",
    "# Seed setup\n",
    "seed = 434\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "train_envs.seed(seed)\n",
    "test_envs.seed(seed)\n",
    "\n",
    "# Model setup\n",
    "input_dim = env.observation_space.shape[1]\n",
    "action_dim = env.action_space.shape[0]\n",
    "preprocess_output_dim = 256\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "net = LSTMFeatureExtractor(input_dim=input_dim, features_dim=preprocess_output_dim)\n",
    "actor = ActorProb(net, action_dim, unbounded=True, device=device, preprocess_net_output_dim=preprocess_output_dim).to(device)\n",
    "critic = Critic(net, device=device, preprocess_net_output_dim=preprocess_output_dim).to(device)\n",
    "actor_critic = ActorCritic(actor, critic)\n",
    "\n",
    "# Orthogonal initialization\n",
    "for m in actor_critic.modules():\n",
    "    if isinstance(m, torch.nn.Linear):\n",
    "        torch.nn.init.orthogonal_(m.weight)\n",
    "        torch.nn.init.zeros_(m.bias)\n",
    "\n",
    "optim = torch.optim.Adam(actor_critic.parameters(), lr=3e-4)\n",
    "\n",
    "policy = PPOPolicy(\n",
    "    actor=actor,\n",
    "    critic=critic,\n",
    "    optim=optim,\n",
    "    dist_fn=dist,\n",
    "    discount_factor=1,\n",
    "    max_grad_norm=0.5,\n",
    "    eps_clip=0.2,\n",
    "    vf_coef=0.5,\n",
    "    ent_coef=0.01,\n",
    "    reward_normalization=True,\n",
    "    advantage_normalization=True,\n",
    "    recompute_advantage=True,\n",
    "    value_clip=True,\n",
    "    gae_lambda=0.95,\n",
    "    action_space=env.action_space,\n",
    ")\n",
    "\n",
    "# Collector setup\n",
    "train_collector = Collector(\n",
    "    policy,\n",
    "    train_envs,\n",
    "    VectorReplayBuffer(20000, len(train_envs))\n",
    ")\n",
    "test_collector = Collector(policy, test_envs)\n",
    "\n",
    "# Logging setup\n",
    "log_path = os.path.join(\"log\", \"forex_trading\", \"ppo\")\n",
    "writer = SummaryWriter(log_path)\n",
    "logger = TensorboardLogger(writer, save_interval=1)\n",
    "\n",
    "# Save best policy\n",
    "save_best_fn = lambda policy: torch.save(policy.state_dict(), os.path.join(log_path, \"policy.pth\"))\n",
    "\n",
    "def stop_fn(mean_rewards: float):\n",
    "    return mean_rewards >= 1e5  # Set an appropriate threshold for stopping\n",
    "\n",
    "def save_checkpoint_fn(epoch: int, env_step: int, gradient_step: int):\n",
    "    ckpt_path = os.path.join(log_path, \"checkpoint.pth\")\n",
    "    torch.save({\n",
    "        \"model\": policy.state_dict(),\n",
    "        \"optim\": optim.state_dict(),\n",
    "    }, ckpt_path)\n",
    "    return ckpt_path\n",
    "\n",
    "# Training\n",
    "trainer = OnpolicyTrainer(\n",
    "    policy=policy,\n",
    "    train_collector=train_collector,\n",
    "    test_collector=test_collector,\n",
    "    max_epoch=50,\n",
    "    step_per_epoch=1000,\n",
    "    repeat_per_collect=10,\n",
    "    episode_per_test=10,\n",
    "    batch_size=64,\n",
    "    episode_per_collect=200,\n",
    "    stop_fn=stop_fn,\n",
    "    save_best_fn=save_best_fn,\n",
    "    logger=logger,\n",
    "    save_checkpoint_fn=save_checkpoint_fn,\n",
    ")\n",
    "\n",
    "for epoch_stat in trainer:\n",
    "    print(f\"Epoch: {epoch_stat.epoch}\")\n",
    "    print(epoch_stat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-01-13T09:07:08.123479Z",
     "iopub.status.idle": "2025-01-13T09:07:08.123870Z",
     "shell.execute_reply": "2025-01-13T09:07:08.123655Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tianshou.data import Collector, VectorReplayBuffer\n",
    "from tianshou.trainer import onpolicy_trainer\n",
    "\n",
    "# Set up training and test collectors\n",
    "train_collector = Collector(policy, envs, VectorReplayBuffer(20000, 10))\n",
    "test_collector = Collector(policy, envs)\n",
    "\n",
    "# Training configuration\n",
    "result = onpolicy_trainer(\n",
    "    policy=policy,\n",
    "    train_collector=train_collector,\n",
    "    test_collector=test_collector,\n",
    "    max_epoch=50,  # Total number of epochs to train\n",
    "    step_per_epoch=1000,  # Total steps per epoch\n",
    "    repeat_per_collect=10,  # Gradient updates per data collection\n",
    "    episode_per_test=10,  # Episodes for testing after each epoch\n",
    "    batch_size=64,  # Batch size for updates\n",
    "    step_per_collect=200,  # Steps per data collection\n",
    "    save_best_fn=lambda policy: torch.save(policy.state_dict(), 'ppo_policy.pth')\n",
    ")\n",
    "\n",
    "# Print training results\n",
    "print(f\"Training complete. Best policy reward: {result['best_reward']}\")\n",
    "\n",
    "# Save the trained policy\n",
    "torch.save(policy.state_dict(), 'ppo_policy_final.pth')\n",
    "\n",
    "# Load the trained policy (if needed)\n",
    "policy.load_state_dict(torch.load('ppo_policy_final.pth'))\n",
    "\n",
    "# --- Evaluation ---\n",
    "# Set up balance tracking\n",
    "balance_history = [[] for _ in range(10)]  # Assuming 10 environments\n",
    "\n",
    "obs = envs.reset()\n",
    "lstm_states = None\n",
    "episode_starts = np.ones((10,), dtype=bool)  # Assuming 10 environments\n",
    "\n",
    "while True:\n",
    "    # Get actions from the policy\n",
    "    action, lstm_states = policy.forward(obs, state=lstm_states, episode_start=episode_starts)\n",
    "    obs, rewards, terminated, truncated, infos = envs.step(action)\n",
    "\n",
    "    # Track balances for each environment\n",
    "    for i, info in enumerate(infos):\n",
    "        balance_history[i].append(info[\"balance\"])\n",
    "\n",
    "    episode_starts = terminated | truncated\n",
    "    if np.any(terminated | truncated):  # Stop if any environment is done\n",
    "        break\n",
    "\n",
    "print(\"Evaluation complete!\")\n",
    "\n",
    "# --- Plot Results ---\n",
    "# Plot the balance history for all environments\n",
    "plt.figure(figsize=(10, 6))\n",
    "for i in range(len(balance_history)):\n",
    "    plt.plot(balance_history[i], label=f\"Env {i + 1}\")\n",
    "\n",
    "plt.title(\"Balance Over Steps for All Environments\")\n",
    "plt.xlabel(\"Steps\")\n",
    "plt.ylabel(\"Balance\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 6418935,
     "sourceId": 10363867,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30822,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
